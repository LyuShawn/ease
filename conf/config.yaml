model_args:
  model_name_or_path: "bert-base-uncased" # bert-base-multilingual-cased
  cache_dir: None
  model_revision: "main"
  use_auth_token: False
  simcse_temp: 0.05
  ease_temp: 0.01
  pooler_type: "cls" # "avg-first_last"
  hard_negative_weight: 1.0
  do_mlm: False
  mlm_weight: 0.1
  mlp_only_train: True
  ease_loss_ratio: 0
  use_entity_transformation: True
  use_only_ease: False
  max_seq_length: 32

train_args:
  seed: 42
  learning_rate: 3e-05
  datasets:
    - "wiki_hyperlink" # wikidata_hyperlink
    - "wiki_first-sentence"
  sample_nums:
    - 100000
    - 100000
  # langs:
  #   - "en"
  #   - "es"
  #   - "ar"
  #   - "tr"
  # not_train: False
  # hard_negative_num: 0
  experiment_name: "9/15_ease"
  output_dir: results/my-unsup-simcse-bert-base-uncased
  _n_gpu: 1
  adafactor: false
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  dataloader_drop_last: false
  dataloader_num_workers: 0
  debug: false
  deepspeed: null
  disable_tqdm: false
  do_eval: false
  do_predict: false
  do_train: false
  eval_accumulation_steps: null
  eval_steps: 500
  # evaluation_strategy: "no"
  fp16: false
  fp16_backend: auto
  fp16_opt_level: O1
  gradient_accumulation_steps: 1
  greater_is_better: null
  ignore_data_skip: false
  label_names: null
  label_smoothing_factor: 0.0
  load_best_model_at_end: false
  local_rank: -1
  logging_dir: runs/Sep10_17-58-29_gpuhost06
  logging_first_step: false
  logging_steps: 500
  lr_scheduler_type: linear
  max_grad_norm: 1.0
  max_steps: -1
  metric_for_best_model: null
  no_cuda: false
  num_train_epochs: 3
  overwrite_output_dir: false
  past_index: -1
  per_device_eval_batch_size: 8
  per_device_train_batch_size: 8
  per_gpu_eval_batch_size: null
  per_gpu_train_batch_size: null
  prediction_loss_only: false
  remove_unused_columns: true
  run_name: results/my-unsup-ease-bert-base-uncased
  save_steps: 500
  save_total_limit: null
  sharded_ddp: false
  tpu_metrics_debug: false
  tpu_num_cores: null
  warmup_steps: 0
  weight_decay: 0.0
