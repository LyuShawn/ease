model_args:
  model_name_or_path: "bert-base-uncased" # bert-base-multilingual-cased
  cache_dir: EASE/cache_dir
  model_revision: "main"
  use_auth_token: False
  simcse_temp: 0.05
  ease_temp: 0.01
  pooler_type: "cls" # "avg-first_last"
  hard_negative_weight: 1.0
  do_mlm: False
  mlp_only_train: True
  mlm_loss_ratio: 0.1
  ease_loss_ratio: 0
  simcse_loss_ratio: 1
  use_entity_transformation: True
  use_another_transformation_for_hn: False
  use_only_ease: False
  max_seq_length: 32
  min_seq_length: 1
  hard_negative_num: 1
  entity_emb_dim: 768
  entity_emb_shape: None
  init_wiki2emb: True
  masked_sentence_ratio: 0
  use_non_linear_transformation: False
  activation: tanh
  use_equal_loss: False
  use_mlp_forcibly: False
  use_entity_to_sentence_loss: False

train_args:
  train_saved_model: False
  seed: 42
  learning_rate: 5e-05
  # dataset_name_or_path: wiki_en # wiki_18 or your dataset path
  dataset_name_or_path: test
  wikipedia2vec_path: data/enwiki.fp16.768.vec
  langs:
    - "en"
  eval_transfer: False
  do_eval: True
  do_train: True
  experiment_name: hoge
  output_dir: results/hoge
  num_train_epochs: 1
  per_device_train_batch_size: 8
  eval_steps: 125
  logging_steps: 125
  fp16: True
  overwrite_output_dir: True
  greater_is_better: True
  metric_for_best_model: stsb_spearman # avg_sts
  load_best_model_at_end: True
  gradient_accumulation_steps: 16
  warmup_steps: 500
  ignore_data_skip: False
  resume_from_checkpoint: False
  group_by_length: False
  use_monolingual_batch: False
